{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "nome = 'Matheus Lindino'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "092ed62a-2c48-43fb-fd7c-35c45df981f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Matheus Lindino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da aula 4, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 80):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente mas fácil de entender) e outra matricial (eficiente mas difícil de entender).\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm_notebook\n",
        "from typing import List\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db13c2f-2ef4-48d4-f92f-bc3b02fc1f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 28 19:48:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1e2f2f-431d-4788-86fa-8d9b0b6ee26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0170c90e-e880-4f0e-e853-79fac18b0da2"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15008b58-b682-466a-f72a-5380d94545bc"
      },
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "This was the first PPV in a new era for the WWE as Hulk Hogan, The Ultimate Warrior, Ric Flair and S\n",
            "This is, in my opinion, a very good film, especially for Michael Jackson lovers. It contains a messa\n",
            "Impressed! This is the worst SRK movie and one of the worst Bollywood movies I ever saw! I didn't li\n",
            "3 últimas amostras treino:\n",
            "Why me? Why should I be subjected to such slaughter of what could have made an interesting plot?! At\n",
            "Ironically for a play unavailable on film or video for so long, ARMS AND THE MAN has remained fairly\n",
            "If any show in the last ten years deserves a 10, it is this rare gem. It allows us to escape back to\n",
            "3 primeiras amostras validação:\n",
            "If you ever have the chance to see Sandra Bernhard live in person, you better move on it sweetie. I \n",
            "This movie is a horrible distortion of lies and exaggerations that were put together by the most sha\n",
            "First off, anyone looking for meaningful \"outcome oriented\" cinema that packs some sort of social me\n",
            "3 últimas amostras validação:\n",
            "Vincente Minnelli directed some of the most celebrated entertainments in cinema history... He was am\n",
            "This movie is a ripoff of James Cain's novel, THE POSTMAN ALWAYS RINGS TWICE. Apparently, the direct\n",
            "A failure. The movie was just not good. It has humor that 5 year olds that will not even giggle at. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Tokenizer"
      ],
      "metadata": {
        "id": "JmSwNW51OlD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self, max_tokens=3000):\n",
        "    self.max_tokens_ = max_tokens\n",
        "    self.vocab_ = None\n",
        "\n",
        "  def tokenize(self, text: str):\n",
        "    pattern = r'\\w+|[^\\w\\s]'\n",
        "    text = text.replace('<br />',' ')\n",
        "    return [tokens.lower() for tokens in re.findall(pattern, text)]\n",
        "\n",
        "  def create_vocab(self, texts: List[str]):\n",
        "    tokens = self.tokenize(' '.join(texts))\n",
        "    counter = collections.Counter(tokens).most_common(self.max_tokens_)\n",
        "    self.vocab_ = dict((key, i) for i, (key, values) in enumerate(counter))\n",
        "    self.vocab_['unk'] = -1\n",
        "    \n",
        "    return self\n",
        "\n",
        "  def encode(self, data: str):\n",
        "    data = self.tokenize(data)\n",
        "    return [self.vocab_.get(sample, -1) for sample in data]\n",
        "  \n",
        "  def decode(self, array):\n",
        "    inverse_vocab = {y: x for x, y in self.vocab_.items()}\n",
        "    return [inverse_vocab[i] for i in array]"
      ],
      "metadata": {
        "id": "jiOjWMp3OiZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asserts da tokenização e codificação"
      ],
      "metadata": {
        "id": "YvfL9398Oo7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['Apple, Banana, Apple, Avocado, Pineapple <br /><br />',\n",
        "          'Apple. Pineapple and Mango',\n",
        "          'Banana, Mango! Banana, Apple',\n",
        "          'Pineapple Berry Apricot Apple',\n",
        "          'Avocado Apple']\n",
        "            \n",
        "sample = 'Apricot Apple, Banana, Banana Banana'\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(max_tokens=3)\n",
        "tokenizer.create_vocab(corpus)\n",
        "vocab = tokenizer.vocab_\n",
        "encoded = tokenizer.encode(sample)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "assert list(vocab.keys()) == ['apple', ',', 'banana', 'unk'], 'Vocab Incorrect'\n",
        "assert encoded == [-1, 0, 1, 2, 1, 2, 2], 'Encoder Incorrect'\n",
        "assert decoded == ['unk', 'apple', ',', 'banana', ',', 'banana', 'banana'], 'Decoder Incorrect'"
      ],
      "metadata": {
        "id": "QxqflehdOmyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "GwSNPh3sOtAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, n_context=9):\n",
        "        super().__init__()\n",
        "        self.data = []\n",
        "              \n",
        "        for sample in data:\n",
        "          sample = tokenizer.encode(sample)\n",
        "          \n",
        "          for i in range(0, len(sample) - n_context):\n",
        "            sample_sliced = sample[i:i+n_context+1]\n",
        "            \n",
        "            if -1 not in sample_sliced:\n",
        "              self.data.append(sample_sliced) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.data[index][:-1]), torch.tensor(self.data[index][-1]).long()"
      ],
      "metadata": {
        "id": "O3G-wu2UOq_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asserts do Dataset"
      ],
      "metadata": {
        "id": "qO3EVBTwOzIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['Apple, Banana, Apple, Avocado, Pineapple',\n",
        "          'Apple. Pineapple and Mango',\n",
        "          'Banana, Mango! Banana, Apple',\n",
        "          'Pineapple Berry Apricot Apple',\n",
        "          'Avocado Apple']\n",
        "\n",
        "n_context = 2\n",
        "customDataset = IMDBDataset(data=corpus, tokenizer=Tokenizer(max_tokens=3).create_vocab(corpus), n_context=n_context)\n",
        "x, y = customDataset.__getitem__(0)\n",
        "\n",
        "assert len(customDataset.data[0]) == n_context+1, 'Dataset shape incorrect'\n",
        "assert x.shape[0] == n_context, 'Example shape incorrect'"
      ],
      "metadata": {
        "id": "WnRHUn8EOuYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network\n",
        "\n"
      ],
      "metadata": {
        "id": "echP_6PbO2jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Attention (FUNCTION)"
      ],
      "metadata": {
        "id": "GCXWQqRLP3Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_self_attention(X, Wq, Wk, Wv, Wo):\n",
        "  Q = torch.matmul(X, Wq)\n",
        "  K = torch.matmul(X, Wk)\n",
        "  V = torch.matmul(X, Wv)\n",
        "  \n",
        "  E = []\n",
        "  for query in Q:\n",
        "    scores = []\n",
        "    for key in K:\n",
        "      scores.append(torch.dot(query, key))\n",
        "    \n",
        "    attention_weights = torch.softmax(torch.FloatTensor(scores), dim=0)\n",
        "\n",
        "    new_E = 0\n",
        "    for weight, value in zip(attention_weights, V):\n",
        "      new_E += weight * value\n",
        "    \n",
        "    new_E = torch.matmul(new_E, Wo)\n",
        "    E.append(new_E)\n",
        "  return torch.stack(E)\n",
        "\n",
        "def matricial_self_attention(X, Wq, Wk, Wv, Wo):\n",
        "  Q = torch.matmul(X, Wq)\n",
        "  K = torch.matmul(X, Wk)\n",
        "  V = torch.matmul(X, Wv)\n",
        "\n",
        "  scores = torch.matmul(Q, K.T)\n",
        "  probs  = torch.softmax(scores, dim=-1)\n",
        "  E      = torch.matmul(probs, V)\n",
        "  \n",
        "  return torch.matmul(E, Wo)"
      ],
      "metadata": {
        "id": "pH8Rr3GJP46d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = torch.FloatTensor([\n",
        "    [0.0, 2.0],\n",
        "    [-1.5, 0.2],\n",
        "    [0.5, 0.6],\n",
        "])\n",
        "\n",
        "E = []\n",
        "\n",
        "Wq = torch.rand((2,2))\n",
        "Wk = torch.rand((2,2))\n",
        "Wv = torch.rand((2,2))\n",
        "Wo = torch.rand((2,2))\n",
        "\n",
        "iterative = iterative_self_attention(seq, Wq, Wk, Wv, Wo)\n",
        "matricial = matricial_self_attention(seq, Wq, Wk, Wv, Wo)\n",
        "\n",
        "iterative = torch.round(iterative, decimals=6)\n",
        "matricial = torch.round(matricial, decimals=6)\n",
        "\n",
        "assert torch.eq(iterative, matricial).all(), 'Wrong implementation'"
      ],
      "metadata": {
        "id": "9dYdN_FBTQqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Attention (CLASS)\n",
        "\n"
      ],
      "metadata": {
        "id": "-DeGMMpFYw2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IterativeSelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wk = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wv = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wo = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "\n",
        "  def forward(self, data):\n",
        "    Q = self.Wq(data)\n",
        "    K = self.Wk(data)\n",
        "    V = self.Wv(data)\n",
        "    \n",
        "    E = []\n",
        "    for query in Q:\n",
        "      scores = []\n",
        "      for key in K:\n",
        "        score = torch.matmul(query, key.t())\n",
        "        scores.append(score.flatten())\n",
        "\n",
        "      attention_weights = torch.softmax(torch.cat(scores), dim=0)\n",
        "\n",
        "      new_E = 0\n",
        "      for weight, value in zip(attention_weights, V):\n",
        "        new_E += weight * value\n",
        "      \n",
        "      new_E = self.Wo(new_E)\n",
        "      E.append(new_E)\n",
        "    return torch.stack(E)\n",
        "\n",
        "class MatricialSelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.Wq = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wk = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wv = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "    self.Wo = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
        "\n",
        "  def forward(self, data):\n",
        "    Q = self.Wq(data)\n",
        "    K = self.Wk(data)\n",
        "    V = self.Wv(data)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(dim0=-2, dim1=-1))\n",
        "    probs  = torch.softmax(scores, dim=-1)\n",
        "    E      = torch.matmul(probs, V)\n",
        "    return self.Wo(E)"
      ],
      "metadata": {
        "id": "SCUOBP7EY1xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "n_context  = 6 \n",
        "vocab_size = 50\n",
        "embedding_dim  = 100\n",
        "\n",
        "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, max_norm=True)\n",
        "matricial = MatricialSelfAttention(embedding_dim=embedding_dim)\n",
        "\n",
        "inputs = torch.rand((batch_size, n_context)).long()\n",
        "inputs = embedding(inputs)\n",
        "E_matricial = matricial(inputs)\n",
        "\n",
        "assert list(E_matricial.shape) == [batch_size, n_context, embedding_dim], 'Self Attention shape incorrect'"
      ],
      "metadata": {
        "id": "z8wL8haWadMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "4-A31eVXcNJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModel(nn.Module):\n",
        "  def __init__(self, n_context, vocab_size, embedding_dim, hidden_size):\n",
        "    super().__init__()\n",
        "    self.embedding      = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, max_norm=True)\n",
        "    self.positional     = nn.Parameter(torch.ones([n_context, embedding_dim]).uniform_(-1, 1))\n",
        "    self.self_attention = MatricialSelfAttention(embedding_dim=embedding_dim)\n",
        "    self.input_layer    = nn.Linear(in_features=n_context*embedding_dim, out_features=hidden_size)\n",
        "    self.activation     = nn.ReLU()\n",
        "    self.out_layer      = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
        "\n",
        "  def forward(self, data):\n",
        "    data = self.embedding(data) + self.positional\n",
        "    data = self.activation(data)\n",
        "    data = self.self_attention(data)\n",
        "    data = self.activation(data)\n",
        "    data = self.input_layer(data.flatten(start_dim=1))\n",
        "    data = self.activation(data)\n",
        "    data = self.out_layer(data)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "NPAjltaMO0Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asserts do modelo"
      ],
      "metadata": {
        "id": "y6f9blnCO4-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "n_context  = 10\n",
        "vocab_size = 50\n",
        "\n",
        "model = AttentionModel(n_context=n_context, vocab_size=vocab_size, embedding_dim=10, hidden_size=10)\n",
        "inputs = torch.rand((batch_size, n_context)).long()\n",
        "logits = model(inputs)\n",
        "\n",
        "assert list(logits.shape) == [batch_size, vocab_size], 'Logits shape incorrect'"
      ],
      "metadata": {
        "id": "KFgTuLvuO3so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "ehe1JpceiA9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping():\n",
        "  def __init__(self, patience=10, min_delta=0):\n",
        "    self.patience = patience\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.best_model_wts = None\n",
        "    self.min_delta = min_delta\n",
        "\n",
        "  def __call__(self, model, val_loss):\n",
        "    score = -val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "        self.best_score = score\n",
        "        self.best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        return False\n",
        "\n",
        "    elif score < self.best_score + self.min_delta:\n",
        "        self.counter += 1\n",
        "        if self.counter >= self.patience:\n",
        "            return True\n",
        "    else:\n",
        "        self.best_score = score\n",
        "        self.best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        self.counter = 0\n",
        "        return False"
      ],
      "metadata": {
        "id": "zq8YBrilhSuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções auxiliares para treinamento"
      ],
      "metadata": {
        "id": "MDXh7_t0iEEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, get_perplexity, initial_perplexity):\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  model.train()\n",
        "  for inputs, targets in dataloader:    \n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    \n",
        "    initial_perplexity = np.exp(loss.item()) if get_perplexity  else initial_perplexity\n",
        "    get_perplexity = False\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() \n",
        "    running_corrects += torch.sum(preds == targets.data)\n",
        "\n",
        "  return running_loss, running_corrects, initial_perplexity\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "\n",
        "  model.eval()\n",
        "  for inputs, targets in dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      \n",
        "      running_loss += loss.item()\n",
        "      running_corrects += torch.sum(preds == targets.data)\n",
        "\n",
        "  return running_loss, running_corrects"
      ],
      "metadata": {
        "id": "Fi1AB752iET-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiper-Parametros"
      ],
      "metadata": {
        "id": "MinNIoTniG4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  'n_context': 9,\n",
        "  'vocab_size': 3000,\n",
        "  'embedding_dim': 100,\n",
        "  'hidden_size': 64,\n",
        "  'batch_size': 256,\n",
        "  'epochs': 100,\n",
        "  'lr': 1e-3,\n",
        "  'patience': 10,\n",
        "  'min_delta':0\n",
        "}"
      ],
      "metadata": {
        "id": "l3O2WXt6iI4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer     = Tokenizer(max_tokens=params['vocab_size']).create_vocab(x_train)\n",
        "train_dataset = IMDBDataset(data=x_train, tokenizer=tokenizer, n_context=params['n_context'])\n",
        "val_dataset   = IMDBDataset(data=x_valid, tokenizer=tokenizer, n_context=params['n_context'])\n",
        "test_dataset  = IMDBDataset(data=x_test, tokenizer=tokenizer, n_context=params['n_context'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
        "test_loader  = DataLoader(test_dataset, batch_size=params['batch_size'])"
      ],
      "metadata": {
        "id": "wdo2xWdViKX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionModel(n_context=params['n_context'], vocab_size=params['vocab_size'], embedding_dim=params['embedding_dim'], hidden_size=params['hidden_size'])\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stopping = EarlyStopping(patience=params['patience'], min_delta=params['min_delta'])\n",
        "\n",
        "history = {'train_loss': [], 'val_loss': [], 'train_acc':[], 'val_acc':[], 'train_perplexity': [], 'val_perplexity' : []}\n",
        "initial_perplexity = 0.0\n",
        "\n",
        "for epoch in tqdm(range(params['epochs'])):\n",
        "  train_loss, train_correct, initial_perplexity = train_epoch(model=model, \n",
        "                                                              dataloader=train_loader, \n",
        "                                                              criterion=criterion, \n",
        "                                                              optimizer=optimizer,\n",
        "                                                              get_perplexity=True if epoch == 0 else False,\n",
        "                                                              initial_perplexity=initial_perplexity)\n",
        "\n",
        "  val_loss, val_correct  = evaluate(model=model, \n",
        "                                    dataloader=val_loader,\n",
        "                                    criterion=criterion)\n",
        "\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "  train_acc = train_correct.cpu().item() / len(train_loader.sampler) * 100\n",
        "  train_perplexity = np.exp(train_loss)\n",
        "\n",
        "  val_loss = val_loss / len(val_loader)\n",
        "  val_acc = val_correct.cpu().item() / len(val_loader.sampler) * 100\n",
        "  val_perplexity = np.exp(val_loss)\n",
        "\n",
        "  if early_stopping(model, val_loss): break\n",
        "  \n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_perplexity'].append(train_perplexity)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_perplexity'].append(val_perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z9ceAX7iL2l",
        "outputId": "8d9925eb-f8ac-419a-f687-ca64e478e976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/100 [02:08<1:09:59, 43.30s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.arange(len(history['train_loss']))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(30,7))\n",
        "axes[0].plot(epochs, history['train_loss'], label='Train')\n",
        "axes[0].plot(epochs, history['val_loss'], label='Validation')\n",
        "axes[0].set_title('Cross Entropy Loss (with Early Stopping)')\n",
        "axes[0].set_xlabel('Epochs'); axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(); axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, history['train_acc'], label='Train')\n",
        "axes[1].plot(epochs, history['val_acc'], label='Validation')\n",
        "axes[1].set_title('Accuracy (with Early Stopping)')\n",
        "axes[1].set_xlabel('Epochs'); axes[1].set_ylabel('Acc')\n",
        "axes[1].grid(); axes[1].legend()\n",
        "\n",
        "axes[2].plot(epochs, history['train_perplexity'], label='Train')\n",
        "axes[2].plot(epochs, history['val_perplexity'], label='Validation')\n",
        "axes[2].set_title('Perplexity (with Early Stopping)')\n",
        "axes[2].set_xlabel('Epochs'); axes[2].set_ylabel('EXP(Loss)')\n",
        "axes[2].grid(); axes[2].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VDr27uz-iPds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionModel(n_context=params['n_context'], vocab_size=params['vocab_size'], embedding_dim=params['embedding_dim'], hidden_size=params['hidden_size'])\n",
        "model.load_state_dict(early_stopping.best_model_wts)\n",
        "model.to(device)\n",
        "\n",
        "test_loss, test_correct  = evaluate(model=model, \n",
        "                                  dataloader=test_loader,\n",
        "                                  criterion=criterion)\n",
        "\n",
        "test_loss = test_loss / len(test_loader)\n",
        "test_acc = test_correct.cpu().item() / len(test_loader.sampler) * 100\n",
        "test_perplexity = np.exp(test_loss)\n",
        "\n",
        "print(f'INITIAL PERPLEXITY: {initial_perplexity} ----- TEST LOSS: {test_loss} ----- TEST ACC: {test_acc} ----- TEST PERPLEXITY: {test_perplexity}')"
      ],
      "metadata": {
        "id": "liKaoMImtL-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FUNÇÃO RETIRADA DO TRABALHO DO ANDERSON PARA MELHORAR AO PLOT DAS FRASES\n",
        "def adjust_string(sample):\n",
        "  preety_sentence = re.sub(r\" \\.\", \".\", sample)\n",
        "  preety_sentence = re.sub(r\" \\,\", \",\", preety_sentence)\n",
        "  preety_sentence = re.sub(r\" \\:\", \":\", preety_sentence)\n",
        "  preety_sentence = re.sub(r\" \\!\", \"!\", preety_sentence)\n",
        "  preety_sentence = re.sub(r\" \\?\", \"?\", preety_sentence)\n",
        "  preety_sentence = re.sub(r\" \\' \", \"'\", preety_sentence)\n",
        "\n",
        "  return preety_sentence"
      ],
      "metadata": {
        "id": "8i3JR5Ylo-pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = \"Amazing movie, good writer, terrific actors and\"\n",
        "\n",
        "for i in range(20):\n",
        "  vector_input = torch.Tensor(tokenizer.encode(text_sample)).long()\n",
        "  vector_input = vector_input.reshape(1,-1).to(device)\n",
        "  \n",
        "  logits = model(vector_input[:, i:])\n",
        "  _, preds = torch.max(logits, 1)\n",
        "\n",
        "  vector_input = torch.cat((vector_input, preds.reshape(1,-1)), dim=1)\n",
        "  text_sample = tokenizer.decode(vector_input.reshape(-1).cpu().numpy())\n",
        "  text_sample = ' '.join(text_sample)\n",
        "\n",
        "  text_sample = adjust_string(text_sample)\n",
        "\n",
        "  print('Generated Text:', text_sample)"
      ],
      "metadata": {
        "id": "rhnTIc_rvenl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUP_tFRppDjQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}